
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>ArXiv Research Digest</title>
        <style>
            
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.5;
    color: #2d3748;
    margin: 0;
    padding: 20px;
    background: #f7fafc;
}

.digest {
    max-width: 800px;
    margin: 0 auto;
    background: white;
    padding: 2rem;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.header {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.header h2 {
    margin: 0;
    color: #2d3748;
}

.header p {
    margin: 0.5rem 0 0;
    color: #718096;
}

.highlights {
    background: #f8fafc;
    padding: 1.5rem;
    border-radius: 8px;
    margin-bottom: 3rem;
    border: 1px solid #e2e8f0;
}

.highlights h3 {
    color: #2d3748;
    margin: 0 0 1rem 0;
    font-size: 1.25rem;
}

.summary {
    white-space: pre-line;
    line-height: 1.6;
}

.summary ol {
    padding-left: 1.5rem;
    margin: 1rem 0;
}

.summary li {
    margin-bottom: 0.75rem;
    padding-left: 0.5rem;
}

.papers {
    margin-top: 3rem;
}

.papers h3 {
    color: #2d3748;
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.paper {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid #e2e8f0;
}

.paper:last-child {
    border-bottom: none;
}

.paper h3 {
    margin: 0 0 0.5rem;
    color: #2d3748;
}

.paper .scores {
    display: flex;
    gap: 1rem;
    margin-bottom: 0.5rem;
}

.paper .score {
    padding: 0.25rem 0.5rem;
    border-radius: 4px;
    font-size: 0.875rem;
}

.paper .relevance {
    background: #e6fffa;
    color: #047481;
}

.paper .importance {
    background: #ebf4ff;
    color: #1a56db;
}

.paper .abstract {
    color: #4a5568;
    margin: 0.5rem 0;
}

.paper .meta {
    font-size: 0.875rem;
    color: #718096;
}

.paper .meta a {
    color: #4299e1;
    text-decoration: none;
}

.paper .meta a:hover {
    text-decoration: underline;
}

        </style>
    </head>
    <body>
        
<div class="digest">
    <div class="header">
        <h2>ArXiv Research Digest</h2>
        <p>Found 7 relevant papers out of 29 new submissions</p>
    </div>
    
    <div class="highlights">
        <h3>ðŸ”¥ Key Highlights</h3>
        Hey boss, found 5 cool papers today that I think you'll love:

1. **Fast-weight Product Key Memory** â€” transforms static PKM into a dynamic "fast-weight" episodic memory that updates via local chunk-level gradient descent, letting models rapidly memorize and retrieve new keyâ€“value pairs and generalize to 128K-token contexts despite training on 4K (avg score 8.5/10; relevance 9/10).  
   Why it matters: this gives a practical path to add scalable, low-overhead episodic memory to long-context LLMs to cut perplexity and handle needle-in-a-haystack retrievals; next step for us is a small integration/bench to measure latency and cost vs. attention-based long context. (paper: http://arxiv.org/abs/2601.00671v1)

2. **Trajectory Guard â€” A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI** â€” a Siamese recurrent autoencoder with a hybrid contrastive+reconstruction loss that detects both "wrong plan for the task" and "malformed plan structure" with F1s ~0.88â€“0.94 and 32 ms inference (avg score 8.5/10; relevance 9/10).  
   Why it matters: itâ€™s a practical, low-latency safety shim you can run inline on agent plans (17â€“27Ã— faster than LLM judges), so we can catch faulty multi-step actions in production; next step is to pilot it in our agent orchestration to block or flag risky plans. (paper: http://arxiv.org/abs/2601.00516v1)

3. **Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model** â€” systematic study showing input-shift detectors (via the DomainSAT toolbox) and a label-free, confidence-based output indicator complement each other for reliable, label-free detection of performance drops in VLMs under shift (avg score 8.0/10; relevance 9/10).  
   Why it matters: for any clinical or safety-critical VLM deployment, combine input-shift alarms with output-confidence signals to reduce blind spots; next step is to run DomainSAT and the confidence indicator on our VLMs and tune alert thresholds before deploying to monitoring dashboards. (paper: http://arxiv.org/abs/2601.00716v1)

4. **DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations** â€” a lightweight DPO variant that estimates sample difficulty via off-the-shelf V+L models and reweights preference pairs to emphasize hard examples, improving hallucination suppression and generalization without extra fine-tuning (avg score 8.0/10; relevance 9/10).  
   Why it matters: an easy-to-adopt tweak to our preference optimization pipeline that targets overfitting on easy pairs and yields better multimodal alignment with little added compute; next step is to run DA-DPO on a slice of our multimodal preference data (project page: https://artanic30.github.io/project_pages/DA-DPO/). (paper: http://arxiv.org/abs/2601.00623v1)

5. **HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts** â€” a federated MoE framework that picks and aggregates client-specific expert subsets based on per-expert importance and an information-bottleneck selection to match device budgets while reducing destructive interference (avg score 7.5/10; relevance 8/10).  
   Why it matters: gives a realistic roadmap to do privacy-preserving personalization on constrained devices by tailoring experts and aggregation, so we can feasibly run LLM fine-tuning across heterogeneous clients; next step is to simulate HFedMoE on our client heterogeneity traces to see convergence and bandwidth wins. (paper: http://arxiv.org/abs/2601.00583v1)

Quick action this week: letâ€™s spike two experiments â€” integrate Trajectory Guard into our agent pipeline for immediate safety gains and run a small FwPKM replication on a long-context model, while scheduling follow-ups for DAâ€‘DPO, DomainSAT checks, and an HFedMoE sim.
    </div>
    
    <div class="papers">
        <h3>ðŸ“„ Detailed Papers</h3>
        
<div class="paper">
    <h3>Fast-weight Product Key Memory</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2601.00671v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2601.00516v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 7/10</span>
    </div>
    <div class="abstract">Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2601.00716v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 7/10</span>
    </div>
    <div class="abstract">Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2601.00623v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 7/10</span>
    </div>
    <div class="abstract">While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2601.00583v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>ECR: Manifold-Guided Semantic Cues for Compact Language Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 7/10</span>
    </div>
    <div class="abstract">Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.   To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.   In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2601.00543v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>The Illusion of Insight in Reasoning Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 7/10</span>
    </div>
    <div class="abstract">Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2601.00514v1" target="_blank">View on arXiv</a>
    </div>
</div>

    </div>
</div>

    </body>
    </html>
    