
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>ArXiv Research Digest</title>
        <style>
            
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.5;
    color: #2d3748;
    margin: 0;
    padding: 20px;
    background: #f7fafc;
}

.digest {
    max-width: 800px;
    margin: 0 auto;
    background: white;
    padding: 2rem;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.header {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.header h2 {
    margin: 0;
    color: #2d3748;
}

.header p {
    margin: 0.5rem 0 0;
    color: #718096;
}

.highlights {
    background: #f8fafc;
    padding: 1.5rem;
    border-radius: 8px;
    margin-bottom: 3rem;
    border: 1px solid #e2e8f0;
}

.highlights h3 {
    color: #2d3748;
    margin: 0 0 1rem 0;
    font-size: 1.25rem;
}

.summary {
    white-space: pre-line;
    line-height: 1.6;
}

.summary ol {
    padding-left: 1.5rem;
    margin: 1rem 0;
}

.summary li {
    margin-bottom: 0.75rem;
    padding-left: 0.5rem;
}

.papers {
    margin-top: 3rem;
}

.papers h3 {
    color: #2d3748;
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.paper {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid #e2e8f0;
}

.paper:last-child {
    border-bottom: none;
}

.paper h3 {
    margin: 0 0 0.5rem;
    color: #2d3748;
}

.paper .scores {
    display: flex;
    gap: 1rem;
    margin-bottom: 0.5rem;
}

.paper .score {
    padding: 0.25rem 0.5rem;
    border-radius: 4px;
    font-size: 0.875rem;
}

.paper .relevance {
    background: #e6fffa;
    color: #047481;
}

.paper .importance {
    background: #ebf4ff;
    color: #1a56db;
}

.paper .abstract {
    color: #4a5568;
    margin: 0.5rem 0;
}

.paper .meta {
    font-size: 0.875rem;
    color: #718096;
}

.paper .meta a {
    color: #4299e1;
    text-decoration: none;
}

.paper .meta a:hover {
    text-decoration: underline;
}

        </style>
    </head>
    <body>
        
<div class="digest">
    <div class="header">
        <h2>ArXiv Research Digest</h2>
        <p>Found 20 relevant papers out of 200 new submissions</p>
    </div>
    
    <div class="highlights">
        <h3>üî• Key Highlights</h3>
        Hey boss, found 5 cool papers today that I think you'll love:
<ol>
<li><b>Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models</b><br>
They show that ‚Äúpoetic‚Äù framing systematically bypasses safety across 25 models ‚Äî handcrafted poems hit ~62% jailbreak success and meta-prompted verse up to 18x higher than prose baselines (some providers >90% ASR).  
This is a high-risk finding (9.5/10 overall, relevance 10/10) ‚Äî we need to add stylistic/adversarial-poetry scenarios to our red-team tests and deploy detector/mitigation strategies ASAP (paper: http://arxiv.org/abs/2511.15304v1).</li>

<li><b>Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic AI Systems</b><br>
They propose CLEAR (Cost, Latency, Efficacy, Assurance, Reliability) and show optimizing for accuracy alone can make agents 4.4‚Äì10.8x more expensive and miss reliability issues that drop repeatability from 60% to 25%.  
This matters for production decisions (9.5/10, relevance 10/10) ‚Äî we should pilot CLEAR in our agent evaluation pipeline to control cost and reliability (results correlate with prod success œÅ=0.83) and integrate the benchmark (http://arxiv.org/abs/2511.14136v1).</li>

<li><b>Parameter Importance-Driven Continual Learning for Foundation Models</b><br>
PIECE selectively updates only ~0.1% of core parameters using Fisher/second‚Äëorder importance estimators to learn new domains without forgetting or growing model size, achieving SOTA continual learning.  
At 9.0/10 (relevance 10/10), this gives a practical path for domain adaptation ‚Äî we should prototype PIECE-style selective fine-tuning on a small foundation model to preserve general capabilities in our verticals (http://arxiv.org/abs/2511.15375v1).</li>

<li><b>Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks</b><br>
This SoK organizes IPI defenses into a five-dimension taxonomy, evaluates representative frameworks, uncovers six root causes of failure, and demonstrates three adaptive attacks that greatly increase success rates.  
Scored 9.0/10 (importance 9/10), it‚Äôs a direct call to audit our function-calling defenses ‚Äî we should map our protections to their taxonomy and test the paper‚Äôs adaptive attacks against our agents to close blind spots (http://arxiv.org/abs/2511.15203v1). </li>

<li><b>On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs</b><br>
Using real anonymized incidents and five prompt techniques, they find on-prem open models lag slightly in accuracy but win on privacy, cost, and data sovereignty for incident classification.  
At 9.0/10 relevance, this suggests a pragmatic hybrid strategy for SOC/CSIRT ‚Äî run a pilot using the paper‚Äôs prompt recipes (PHP, SHP, HTP, PRP, ZSL) to see if an on‚Äëprem model meets our privacy/cost targets before committing to a proprietary option (http://arxiv.org/abs/2511.14908v1). </li>
</ol>

Let‚Äôs prioritize: add poetic-jailbreak tests to the red team, adopt CLEAR in our agent benchmarks, and spin up two quick pilots (PIECE fine-tune + on‚Äëprem SOC classifier) this week so we can surface actionable gaps fast.
    </div>
    
    <div class="papers">
        <h3>üìÑ Detailed Papers</h3>
        
<div class="paper">
    <h3>Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 10/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.15304v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic AI Systems</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 10/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Current agentic AI benchmarks predominantly evaluate task completion accuracy, while overlooking critical enterprise requirements such as cost-efficiency, reliability, and operational stability. Through systematic analysis of 12 main benchmarks and empirical evaluation of state-of-the-art agents, we identify three fundamental limitations: (1) absence of cost-controlled evaluation leading to 50x cost variations for similar precision, (2) inadequate reliability assessment where agent performance drops from 60\% (single run) to 25\% (8-run consistency), and (3) missing multidimensional metrics for security, latency, and policy compliance. We propose \textbf{CLEAR} (Cost, Latency, Efficacy, Assurance, Reliability), a holistic evaluation framework specifically designed for enterprise deployment. Evaluation of six leading agents on 300 enterprise tasks demonstrates that optimizing for accuracy alone yields agents 4.4-10.8x more expensive than cost-aware alternatives with comparable performance. Expert evaluation (N=15) confirms that CLEAR better predicts production success (correlation $œÅ=0.83$) compared to accuracy-only evaluation ($œÅ=0.41$).</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.14136v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Parameter Importance-Driven Continual Learning for Foundation Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 10/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.15375v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.15203v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 10/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.14908v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 10/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.14130v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.15557v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>A Compliance-Preserving Retrieval System for Aircraft MRO Task Search</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.15383v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.15169v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.15137v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>MAIF: Enforcing AI Trust and Provenance with an Artifact-Centric Agentic Paradigm</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">The AI trustworthiness crisis threatens to derail the artificial intelligence revolution, with regulatory barriers, security vulnerabilities, and accountability gaps preventing deployment in critical domains. Current AI systems operate on opaque data structures that lack the audit trails, provenance tracking, or explainability required by emerging regulations like the EU AI Act. We propose an artifact-centric AI agent paradigm where behavior is driven by persistent, verifiable data artifacts rather than ephemeral tasks, solving the trustworthiness problem at the data architecture level. Central to this approach is the Multimodal Artifact File Format (MAIF), an AI-native container embedding semantic representations, cryptographic provenance, and granular access controls. MAIF transforms data from passive storage into active trust enforcement, making every AI operation inherently auditable. Our production-ready implementation demonstrates ultra-high-speed streaming (2,720.7 MB/s), optimized video processing (1,342 MB/s), and enterprise-grade security. Novel algorithms for cross-modal attention, semantic compression, and cryptographic binding achieve up to 225 compression while maintaining semantic fidelity. Advanced security features include stream-level access control, real-time tamper detection, and behavioral anomaly analysis with minimal overhead. This approach directly addresses the regulatory, security, and accountability challenges preventing AI deployment in sensitive domains, offering a viable path toward trustworthy AI systems at scale.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.15097v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.   In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.   OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.15061v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressive compression. So we present DynaExq, a runtime system that treats expert precision as a first-class, dynamically managed resource. DynaExq combines (1) a hotness-aware precision controller that continuously aligns expert bit-widths with long-term activation statistics, (2) a fully asynchronous precision-switching pipeline that overlaps promotion and demotion with MoE computation, and (3) a fragmentation-free memory pooling mechanism that supports hybrid-precision experts with deterministic allocation. Together, these components enable stable, non-blocking precision transitions under strict HBM budgets.   Across Qwen3-30B and Qwen3-80B MoE models and six representative benchmarks, DynaExq deploys large LLMs on single RTX 5090 and A6000 GPUs and improves accuracy by up to 4.03 points over static low-precision baselines. The results show that adaptive, workload-aware quantization is an effective strategy for memory-constrained MoE serving.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.15015v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.14846v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental "grammar" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.14694v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Failure to Mix: Large language models struggle to answer according to desired probability distributions</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of "1" 49% of the time produces an answer of "0" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.14630v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.14584v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.14334v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.14210v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2511.14169v1" target="_blank">View on arXiv</a>
    </div>
</div>

    </div>
</div>

    </body>
    </html>
    