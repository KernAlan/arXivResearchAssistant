
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>ArXiv Research Digest</title>
        <style>
            
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.5;
    color: #2d3748;
    margin: 0;
    padding: 20px;
    background: #f7fafc;
}

.digest {
    max-width: 800px;
    margin: 0 auto;
    background: white;
    padding: 2rem;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.header {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.header h2 {
    margin: 0;
    color: #2d3748;
}

.header p {
    margin: 0.5rem 0 0;
    color: #718096;
}

.highlights {
    background: #f8fafc;
    padding: 1.5rem;
    border-radius: 8px;
    margin-bottom: 3rem;
    border: 1px solid #e2e8f0;
}

.highlights h3 {
    color: #2d3748;
    margin: 0 0 1rem 0;
    font-size: 1.25rem;
}

.summary {
    white-space: pre-line;
    line-height: 1.6;
}

.summary ol {
    padding-left: 1.5rem;
    margin: 1rem 0;
}

.summary li {
    margin-bottom: 0.75rem;
    padding-left: 0.5rem;
}

.papers {
    margin-top: 3rem;
}

.papers h3 {
    color: #2d3748;
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.paper {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid #e2e8f0;
}

.paper:last-child {
    border-bottom: none;
}

.paper h3 {
    margin: 0 0 0.5rem;
    color: #2d3748;
}

.paper .scores {
    display: flex;
    gap: 1rem;
    margin-bottom: 0.5rem;
}

.paper .score {
    padding: 0.25rem 0.5rem;
    border-radius: 4px;
    font-size: 0.875rem;
}

.paper .relevance {
    background: #e6fffa;
    color: #047481;
}

.paper .importance {
    background: #ebf4ff;
    color: #1a56db;
}

.paper .abstract {
    color: #4a5568;
    margin: 0.5rem 0;
}

.paper .meta {
    font-size: 0.875rem;
    color: #718096;
}

.paper .meta a {
    color: #4299e1;
    text-decoration: none;
}

.paper .meta a:hover {
    text-decoration: underline;
}

        </style>
    </head>
    <body>
        
<div class="digest">
    <div class="header">
        <h2>ArXiv Research Digest</h2>
        <p>Found 20 relevant papers out of 175 new submissions</p>
    </div>
    
    <div class="highlights">
        <h3>ðŸ”¥ Key Highlights</h3>
        Hey boss, found 5 cool papers today that I think you'll love:

<ol>
<li>Researchers introduced ASPD, a new decoding method for large language models that boosts inference speed by up to 3.19x without sacrificing quality. This means faster, more efficient AI responses, perfect for customer service and real-time applications!</li>

<li>The team at Google created MAGUS, a multi-agent framework that enhances multimodal understanding and generation. It allows different AI models to collaborate, making it easier to switch between text, images, and audio seamlesslyâ€”ideal for developing versatile AI tools.</li>

<li>Tencent rolled out WeChat-YATT, a scalable RLHF training system that improves how we train AI models using human feedback. Its dynamic resource management and efficiency could lead to better-performing models in less time, great for large-scale applications!</li>

<li>This study dives into why open-source LLMs struggle with data analysis, providing insights that could help improve their reasoning abilities. The findings pave the way for enhancements in analytical tasks, making open-source models more competent in real-world scenarios.</li>

<li>Finally, thereâ€™s FreeKV, a clever new system that speeds up KV cache retrieval for LLMs while keeping accuracy intact. It could revolutionize how we handle long context inputs, making AI applications smoother and more efficient.</li>
</ol>

Let me know if you want to chat more about any of these!
    </div>
    
    <div class="papers">
        <h3>ðŸ“„ Detailed Papers</h3>
        
<div class="paper">
    <h3>ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic   Parallelism in LLMs</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 10/10</span>
    </div>
    <div class="abstract">The increasing scale and complexity of large language models (LLMs) pose significant inference latency challenges, primarily due to their autoregressive decoding paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel decoding) can significantly improve the overall inference speed of LLMs. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel decoding modes while maintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant acceleration without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient LLM parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.08895v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>A Unified Multi-Agent Framework for Universal Multimodal Understanding   and Generation</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Real-world multimodal applications often require any-to-any capabilities, enabling both understanding and generation across modalities including text, image, audio, and video. However, integrating the strengths of autoregressive language models (LLMs) for reasoning and diffusion models for high-fidelity generation remains challenging. Existing approaches rely on rigid pipelines or tightly coupled architectures, limiting flexibility and scalability. We propose MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that unifies multimodal understanding and generation via two decoupled phases: Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration within a shared textual workspace. In the Cognition phase, three role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector - engage in collaborative dialogue to perform structured understanding and planning. The Deliberation phase incorporates a Growth-Aware Search mechanism that orchestrates LLM-based reasoning and diffusion-based generation in a mutually reinforcing manner. MAGUS supports plug-and-play extensibility, scalable any-to-any modality conversion, and semantic alignment - all without the need for joint training. Experiments across multiple benchmarks, including image, video, and audio generation, as well as cross-modal instruction following, demonstrate that MAGUS outperforms strong baselines and state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the powerful closed-source model GPT-4o.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.10494v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent paradigm for training large language models and multimodal systems. Despite notable advances enabled by existing RLHF training frameworks, significant challenges remain in scaling to complex multimodal workflows and adapting to dynamic workloads. In particular, current systems often encounter limitations related to controller scalability when managing large models, as well as inefficiencies in orchestrating intricate RLHF pipelines, especially in scenarios that require dynamic sampling and resource allocation. In this paper, we introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple, scalable, and balanced RLHF training framework specifically designed to address these challenges. WeChat-YATT features a parallel controller programming model that enables flexible and efficient orchestration of complex RLHF workflows, effectively mitigating the bottlenecks associated with centralized controller architectures and facilitating scalability in large-scale data scenarios. In addition, we propose a dynamic placement schema that adaptively partitions computational resources and schedules workloads, thereby significantly reducing hardware idle time and improving GPU utilization under variable training conditions. We evaluate WeChat-YATT across a range of experimental scenarios, demonstrating that it achieves substantial improvements in throughput compared to state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been successfully deployed to train models supporting WeChat product features for a large-scale user base, underscoring its effectiveness and robustness in real-world applications.We have open-source WeChat-YATT at https://www.github.com/tencent/WeChat-YATT.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.07970v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic   Empirical Study</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2506.19794v4" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2505.13109v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Searching for Privacy Risks in LLM Agents via Simulation</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">The widespread deployment of LLM-based agents is likely to introduce a critical privacy threat: malicious agents that proactively engage others in multi-turn interactions to extract sensitive information. These dynamic dialogues enable adaptive attack strategies that can cause severe privacy violations, yet their evolving nature makes it difficult to anticipate and discover sophisticated vulnerabilities manually. To tackle this problem, we present a search-based framework that alternates between improving attacker and defender instructions by simulating privacy-critical agent interactions. Each simulation involves three roles: data subject, data sender, and data recipient. While the data subject's behavior is fixed, the attacker (data recipient) attempts to extract sensitive information from the defender (data sender) through persistent and interactive exchanges. To explore this interaction space efficiently, our search algorithm employs LLMs as optimizers, using parallel search with multiple threads and cross-thread propagation to analyze simulation trajectories and iteratively propose new instructions. Through this process, we find that attack strategies escalate from simple direct requests to sophisticated multi-turn tactics such as impersonation and consent forgery, while defenses advance from rule-based constraints to identity-verification state machines. The discovered attacks and defenses transfer across diverse scenarios and backbone models, demonstrating strong practical utility for building privacy-aware agents.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.10880v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Reinforced Language Models for Sequential Decision Making</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) show potential as sequential decision-making agents, but their application is often limited due to a reliance on large, computationally expensive models. This creates a need to improve smaller models, yet existing post-training methods are designed for single-turn interactions and cannot handle credit assignment in multi-step agentic tasks. To address this, we introduce Multi-Step Group-Relative Policy Optimization (MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP) frameworks. For credit assignment, MS-GRPO attributes the entire cumulative episode reward to each individual episode step. We supplement this algorithm with a novel absolute-advantage-weighted episode sampling strategy that we show improves training performance. We evaluate our approach by post-training a 3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate that the method is effective in improving decision-making performance: our post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on the Frozen Lake task. This work demonstrates that targeted post-training is a practical and efficient alternative to relying on model scale for creating sequential decision-making agents using LLMs.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.10839v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in   Clinical Natural Language Inference</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large language models are often assumed to acquire increasingly structured, generalizable internal representations simply by scaling data and parameters. We interrogate this assumption by introducing a Clinical Trial Natural Language Inference benchmark comprising four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probe, allowing us to dissociate failures of factual access from failures of inference. We evaluate six contemporary LLMs under both direct and chain of thought prompting.   Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy, output inferences are highly consistent across samples (mean 0.87), indicating a systematic application of underlying heuristics and shortcuts.   These results reveal fundamental structural and representational limitations: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably (e.g., integrating constraints, weighing evidence, or simulating counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this dissociation explicit and measurable, providing an effective framework for probing the reliability of LLMs in high-stakes domains.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.10777v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and   Adaptive Chest X-Ray Reasoning</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.10501v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Reverse Physician-AI Relationship: Full-process Clinical Diagnosis   Driven by a Large Language Model</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Full-process clinical diagnosis in the real world encompasses the entire diagnostic workflow that begins with only an ambiguous chief complaint. While artificial intelligence (AI), particularly large language models (LLMs), is transforming clinical diagnosis, its role remains largely as an assistant to physicians. This AI-assisted working pattern makes AI can only answer specific medical questions at certain parts within the diagnostic process, but lack the ability to drive the entire diagnostic process starting from an ambiguous complaint, which still relies heavily on human physicians. This gap limits AI's ability to fully reduce physicians' workload and enhance diagnostic efficiency. To address this, we propose a paradigm shift that reverses the relationship between physicians and AI: repositioning AI as the primary director, with physicians serving as its assistants. So we present DxDirector-7B, an LLM endowed with advanced deep thinking capabilities, enabling it to drive the full-process diagnosis with minimal physician involvement. Furthermore, DxDirector-7B establishes a robust accountability framework for misdiagnoses, delineating responsibility between AI and human physicians. In evaluations across rare, complex, and real-world cases under full-process diagnosis setting, DxDirector-7B not only achieves significant superior diagnostic accuracy but also substantially reduces physician workload than state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained analyses across multiple clinical departments and tasks validate its efficacy, with expert evaluations indicating its potential to serve as a viable substitute for medical specialists. These findings mark a new era where AI, traditionally a physicians' assistant, now drives the entire diagnostic process to drastically reduce physicians' workload, indicating an efficient and accurate diagnostic solution.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.10492v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long   Narrative Reasoning</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.10419v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and   Hierarchical Retrieval</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large Language Models by leveraging external knowledge, whereas the effectiveness is often compromised by the retrieval of contextually flawed or incomplete information. To address this, knowledge graph-based RAG methods have evolved towards hierarchical structures, organizing knowledge into multi-level summaries. However, these approaches still suffer from two critical, unaddressed challenges: high-level conceptual summaries exist as disconnected ``semantic islands'', lacking the explicit relations needed for cross-community reasoning; and the retrieval process itself remains structurally unaware, often degenerating into an inefficient flat search that fails to exploit the graph's rich topology. To overcome these limitations, we introduce LeanRAG, a framework that features a deeply collaborative design combining knowledge aggregation and retrieval strategies. LeanRAG first employs a novel semantic aggregation algorithm that forms entity clusters and constructs new explicit relations among aggregation-level summaries, creating a fully navigable semantic network. Then, a bottom-up, structure-guided retrieval strategy anchors queries to the most relevant fine-grained entities and then systematically traverses the graph's semantic pathways to gather concise yet contextually comprehensive evidence sets. The LeanRAG can mitigate the substantial overhead associated with path retrieval on graphs and minimizes redundant information retrieval. Extensive experiments on four challenging QA benchmarks with different domains demonstrate that LeanRAG significantly outperforming existing methods in response quality while reducing 46\% retrieval redundancy. Code is available at: https://github.com/RaZzzyz/LeanRAG</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.10391v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>What to Ask Next? Probing the Imaginative Reasoning of LLMs with   TurtleSoup Puzzles</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">We investigate the capacity of Large Language Models (LLMs) for imaginative reasoning--the proactive construction, testing, and revision of hypotheses in information-sparse environments. Existing benchmarks, often static or focused on social deduction, fail to capture the dynamic, exploratory nature of this reasoning process. To address this gap, we introduce a comprehensive research framework based on the classic "Turtle Soup" game, integrating a benchmark, an agent, and an evaluation protocol. We present TurtleSoup-Bench, the first large-scale, bilingual, interactive benchmark for imaginative reasoning, comprising 800 turtle soup puzzles sourced from both the Internet and expert authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs' performance in this setting. To evaluate reasoning quality, we develop a multi-dimensional protocol measuring logical consistency, detail completion, and conclusion alignment. Experiments with leading LLMs reveal clear capability limits, common failure patterns, and a significant performance gap compared to humans. Our work offers new insights into LLMs' imaginative reasoning and establishes a foundation for future research on exploratory agent behavior.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.10358v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating   Hallucinations in LVLMs</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large Vision-Language Models (LVLMs) have shown strong performance across multimodal tasks. However, they often produce hallucinations -- text that is inconsistent with visual input, due to the limited ability to verify information in different regions of the image. To address this, we propose Multi-Region Fusion Decoding (MRFD), a training-free decoding method that improves factual grounding by modeling inter-region consistency. MRFD identifies salient regions using cross-attention, generates initial responses for each, and computes reliability weights based on Jensen-Shannon Divergence (JSD) among the responses. These weights guide a consistency-aware fusion of per-region predictions, using region-aware prompts inspired by Chain-of-Thought reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD significantly reduces hallucinations and improves response factuality without requiring model updates.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.10264v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Mathematical Computation and Reasoning Errors by Large Language Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) are increasingly utilized in AI-driven educational instruction and assessment, particularly within mathematics education. The capability of LLMs to generate accurate answers and detailed solutions for math problem-solving tasks is foundational for ensuring reliable and precise feedback and assessment in math education practices. Our study focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1, DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including arithmetic, algebra, and number theory, and identifies step-level reasoning errors within their solutions. Instead of relying on standard benchmarks, we intentionally build math tasks (via item models) that are challenging for LLMs and prone to errors. The accuracy of final answers and the presence of errors in individual solution steps were systematically analyzed and coded. Both single-agent and dual-agent configurations were tested. It is observed that the reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly perfect accuracy across all three math task categories. Analysis of errors revealed that procedural slips were the most frequent and significantly impacted overall performance, while conceptual misunderstandings were less frequent. Deploying dual-agent configurations substantially improved overall performance. These findings offer actionable insights into enhancing LLM performance and underscore effective strategies for integrating LLMs into mathematics education, thereby advancing AI-driven instructional practices and assessment precision.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.09932v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>OpenCUA: Open Foundations for Computer-Use Agents</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state-action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-32B achieves an average success rate of 34.8% on OSWorld-Verified, establishing a new state-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA (GPT-4o). Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.09123v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. Biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasing particularly challenging. To address this, we introduce BiasGym, a simple, cost-effective, and generalizable framework for reliably injecting, analyzing, and mitigating conceptual associations within LLMs. BiasGym consists of two components: BiasInject, which injects specific biases into the model via token-based fine-tuning while keeping the model frozen, and BiasScope, which leverages these injected signals to identify and steer the components responsible for biased behavior. Our method enables consistent bias elicitation for mechanistic analysis, supports targeted debiasing without degrading performance on downstream tasks, and generalizes to biases unseen during token-based fine-tuning. We demonstrate the effectiveness of BiasGym in reducing real-world stereotypes (e.g., people from Italy being `reckless drivers') and in probing fictional associations (e.g., people from a fictional country having `blue skin'), showing its utility for both safety interventions and interpretability research.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.08855v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Shuffle-R1: Efficient RL framework for Multimodal Large Language Models   via Data-centric Dynamic Shuffle</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.05612v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Towards Embodied Agentic AI: Review and Classification of LLM- and   VLM-Driven Robot Autonomy and Interaction</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper focuses on those works advancing towards agentic applications and architectures. This includes initial efforts exploring GPT-style interfaces to tooling, as well as more complex system where AI agents are coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2508.05294v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models. Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking. However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks. Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks. For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy. In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance. This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal prompting strategies for LLM-as-a-Judge. We find that using pair-wise comparison outperforms scalar point-wise judging. Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2507.10535v2" target="_blank">View on arXiv</a>
    </div>
</div>

    </div>
</div>

    </body>
    </html>
    