
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>ArXiv Research Digest</title>
        <style>
            
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.5;
    color: #2d3748;
    margin: 0;
    padding: 20px;
    background: #f7fafc;
}

.digest {
    max-width: 800px;
    margin: 0 auto;
    background: white;
    padding: 2rem;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.header {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.header h2 {
    margin: 0;
    color: #2d3748;
}

.header p {
    margin: 0.5rem 0 0;
    color: #718096;
}

.highlights {
    background: #f8fafc;
    padding: 1.5rem;
    border-radius: 8px;
    margin-bottom: 3rem;
    border: 1px solid #e2e8f0;
}

.highlights h3 {
    color: #2d3748;
    margin: 0 0 1rem 0;
    font-size: 1.25rem;
}

.summary {
    white-space: pre-line;
    line-height: 1.6;
}

.summary ol {
    padding-left: 1.5rem;
    margin: 1rem 0;
}

.summary li {
    margin-bottom: 0.75rem;
    padding-left: 0.5rem;
}

.papers {
    margin-top: 3rem;
}

.papers h3 {
    color: #2d3748;
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.paper {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid #e2e8f0;
}

.paper:last-child {
    border-bottom: none;
}

.paper h3 {
    margin: 0 0 0.5rem;
    color: #2d3748;
}

.paper .scores {
    display: flex;
    gap: 1rem;
    margin-bottom: 0.5rem;
}

.paper .score {
    padding: 0.25rem 0.5rem;
    border-radius: 4px;
    font-size: 0.875rem;
}

.paper .relevance {
    background: #e6fffa;
    color: #047481;
}

.paper .importance {
    background: #ebf4ff;
    color: #1a56db;
}

.paper .abstract {
    color: #4a5568;
    margin: 0.5rem 0;
}

.paper .meta {
    font-size: 0.875rem;
    color: #718096;
}

.paper .meta a {
    color: #4299e1;
    text-decoration: none;
}

.paper .meta a:hover {
    text-decoration: underline;
}

        </style>
    </head>
    <body>
        
<div class="digest">
    <div class="header">
        <h2>ArXiv Research Digest</h2>
        <p>Found 20 relevant papers out of 200 new submissions</p>
    </div>
    
    <div class="highlights">
        <h3>ðŸ”¥ Key Highlights</h3>
        Hey boss, I found a few cool papers today that I think you'll love:

<ol>
<li>A team developed a no-code system that allows medical professionals to use powerful language models right on their desktops for kidney disease support. This means they can get accurate, enhanced medical insights without needing tech expertise or worrying about data privacy. Super practical for clinics!</li>
<li>Researchers created a new language model that can control how long it thinks before giving an answer. This lets users balance accuracy and computing costs, meaning they can get better results while saving resources. It's all about efficiency!</li>
<li>Another group is envisioning a future where large language model applications work seamlessly across different platforms. They propose a new architecture that enhances modularity and security, making it easier to develop and integrate AI solutions. This could really help in creating more robust AI systems!</li>
<li>One paper dives into the limitations of reasoning in language models, showing how they can struggle with novel data. They've introduced a benchmark to assess these issues, which could lead to stronger and more reliable reasoning capabilities in future models. Definitely something to keep an eye on!</li>
<li>Finally, there's a new method for enhancing language model reasoning by smartly deciding when to explore new responses versus refining existing ones. This approach has shown improved performance on complex tasks, which could be a game changer for applications that need high accuracy.</li>
</ol>

Let me know if you want to chat more about any of these!
    </div>
    
    <div class="papers">
        <h3>ðŸ“„ Detailed Papers</h3>
        
<div class="paper">
    <h3>KidneyTalk-open: No-code Deployment of a Private Large Language Model   with Medical Documentation-Enhanced Knowledge Database for Kidney Disease</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 10/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Privacy-preserving medical decision support for kidney disease requires localized deployment of large language models (LLMs) while maintaining clinical reasoning capabilities. Current solutions face three challenges: 1) Cloud-based LLMs pose data security risks; 2) Local model deployment demands technical expertise; 3) General LLMs lack mechanisms to integrate medical knowledge. Retrieval-augmented systems also struggle with medical document processing and clinical usability. We developed KidneyTalk-open, a desktop system integrating three technical components: 1) No-code deployment of state-of-the-art (SOTA) open-source LLMs (such as DeepSeek-r1, Qwen2.5) via local inference engine; 2) Medical document processing pipeline combining context-aware chunking and intelligent filtering; 3) Adaptive Retrieval and Augmentation Pipeline (AddRep) employing agents collaboration for improving the recall rate of medical documents. A graphical interface was designed to enable clinicians to manage medical documents and conduct AI-powered consultations without technical expertise. Experimental validation on 1,455 challenging nephrology exam questions demonstrates AddRep's effectiveness: achieving 29.1% accuracy (+8.1% over baseline) with intelligent knowledge integration, while maintaining robustness through 4.9% rejection rate to suppress hallucinations. Comparative case studies with the mainstream products (AnythingLLM, Chatbox, GPT4ALL) demonstrate KidneyTalk-open's superior performance in real clinical query. KidneyTalk-open represents the first no-code medical LLM system enabling secure documentation-enhanced medical Q&A on desktop. Its designs establishes a new framework for privacy-sensitive clinical AI applications. The system significantly lowers technical barriers while improving evidence traceability, enabling more medical staff or patients to use SOTA open-source LLMs conveniently.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04153v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>L1: Controlling How Long A Reasoning Model Thinks With Reinforcement   Learning</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Reasoning language models have shown an uncanny ability to improve performance at test-time by ``thinking longer''-that is, by generating longer chain-of-thought sequences and hence using more compute. However, the length of their chain-of-thought reasoning is not controllable, making it impossible to allocate test-time compute to achieve a desired level of performance. We introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints. We use LCPO to train L1, a reasoning language model that produces outputs satisfying a length constraint given in its prompt. L1's length control allows for smoothly trading off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control. Furthermore, we uncover an unexpected short chain-of-thought capability in models trained with LCPO. For instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise control over reasoning length, allowing for fine-grained allocation of test-time compute and accuracy. We release code and models at https://www.cmu-l3.github.io/l1</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04697v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>The Next Frontier of LLM Applications: Open Ecosystems and Hardware   Synergy</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large Language Model (LLM) applications, including LLM app stores and autonomous agents, are shaping the future of AI ecosystems. However, platform silos, fragmented hardware integration, and the absence of standardized interfaces limit scalability, interoperability, and resource efficiency. While LLM app stores democratize AI, their closed ecosystems restrict modular AI reuse and cross-platform portability. Meanwhile, agent-based frameworks offer flexibility but often lack seamless integration across diverse environments. This paper envisions the future of LLM applications and proposes a three-layer decoupled architecture grounded in software engineering principles such as layered system design, service-oriented architectures, and hardware-software co-design. This architecture separates application logic, communication protocols, and hardware execution, enhancing modularity, efficiency, and cross-platform compatibility. Beyond architecture, we highlight key security and privacy challenges for safe, scalable AI deployment and outline research directions in software and security engineering. This vision aims to foster open, secure, and interoperable LLM ecosystems, guiding future advancements in AI applications.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04596v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Benchmarking Reasoning Robustness in Large Language Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Despite the recent success of large language models (LLMs) in reasoning such as DeepSeek, we for the first time identify a key dilemma in reasoning robustness and generalization: significant performance degradation on novel or incomplete data, suggesting a reliance on memorized patterns rather than systematic reasoning. Our closer examination reveals four key unique limitations underlying this issue:(1) Positional bias--models favor earlier queries in multi-query inputs but answering the wrong one in the latter (e.g., GPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction sensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series and by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical fragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from 97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5 percent); and (4) Memory dependence--models resort to guesswork when missing critical data. These findings further highlight the reliance on heuristic recall over rigorous logical inference, demonstrating challenges in reasoning robustness. To comprehensively investigate these robustness challenges, this paper introduces a novel benchmark, termed as Math-RoB, that exploits hallucinations triggered by missing information to expose reasoning gaps. This is achieved by an instruction-based approach to generate diverse datasets that closely resemble training distributions, facilitating a holistic robustness assessment and advancing the development of more robust reasoning frameworks. Bad character(s) in field Abstract.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04550v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive   Branching Tree Search</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Recent advances demonstrate that increasing inference-time computation can significantly boost the reasoning capabilities of large language models (LLMs). Although repeated sampling (i.e., generating multiple candidate outputs) is a highly effective strategy, it does not leverage external feedback signals for refinement, which are often available in tasks like coding. In this work, we propose $\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, a novel inference-time framework that generalizes repeated sampling with principled multi-turn exploration and exploitation. At each node in the search tree, AB-MCTS dynamically decides whether to "go wider" by expanding new candidate responses or "go deeper" by revisiting existing ones based on external feedback signals. We evaluate our method on complex coding and engineering tasks using frontier models. Empirical results show that AB-MCTS consistently outperforms both repeated sampling and standard MCTS, underscoring the importance of combining the response diversity of LLMs with multi-turn solution refinement for effective inference-time scaling.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04412v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Malware Detection at the Edge with Lightweight LLMs: A Performance   Evaluation</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">The rapid evolution of malware attacks calls for the development of innovative detection methods, especially in resource-constrained edge computing. Traditional detection techniques struggle to keep up with modern malware's sophistication and adaptability, prompting a shift towards advanced methodologies like those leveraging Large Language Models (LLMs) for enhanced malware detection. However, deploying LLMs for malware detection directly at edge devices raises several challenges, including ensuring accuracy in constrained environments and addressing edge devices' energy and computational limits. To tackle these challenges, this paper proposes an architecture leveraging lightweight LLMs' strengths while addressing limitations like reduced accuracy and insufficient computational power. To evaluate the effectiveness of the proposed lightweight LLM-based approach for edge computing, we perform an extensive experimental evaluation using several state-of-the-art lightweight LLMs. We test them with several publicly available datasets specifically designed for edge and IoT scenarios and different edge nodes with varying computational power and characteristics.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04302v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>InterChat: Enhancing Generative Visual Analytics using Multimodal   Interactions</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">The rise of Large Language Models (LLMs) and generative visual analytics systems has transformed data-driven insights, yet significant challenges persist in accurately interpreting users' analytical and interaction intents. While language inputs offer flexibility, they often lack precision, making the expression of complex intents inefficient, error-prone, and time-intensive. To address these limitations, we investigate the design space of multimodal interactions for generative visual analytics through a literature review and pilot brainstorming sessions. Building on these insights, we introduce a highly extensible workflow that integrates multiple LLM agents for intent inference and visualization generation. We develop InterChat, a generative visual analytics system that combines direct manipulation of visual elements with natural language inputs. This integration enables precise intent communication and supports progressive, visually driven exploratory data analyses. By employing effective prompt engineering, and contextual interaction linking, alongside intuitive visualization and interaction designs, InterChat bridges the gap between user interactions and LLM-driven visualizations, enhancing both interpretability and usability. Extensive evaluations, including two usage scenarios, a user study, and expert feedback, demonstrate the effectiveness of InterChat. Results show significant improvements in the accuracy and efficiency of handling complex visual analytics tasks, highlighting the potential of multimodal interactions to redefine user engagement and analytical depth in generative visual analytics.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04110v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>ReasonGraph: Visualisation of Reasoning Paths</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) reasoning processes are challenging to analyze due to their complexity and the lack of organized visualization tools. We present ReasonGraph, a web-based platform for visualizing and analyzing LLM reasoning processes. It supports both sequential and tree-based reasoning methods while integrating with major LLM providers and over fifty state-of-the-art models. ReasonGraph incorporates an intuitive UI with meta reasoning method selection, configurable visualization parameters, and a modular framework that facilitates efficient extension. Our evaluation shows high parsing reliability, efficient processing, and strong usability across various downstream applications. By providing a unified visualization framework, ReasonGraph reduces cognitive load in analyzing complex reasoning paths, improves error detection in logical processes, and enables more effective development of LLM-based applications. The platform is open-source, promoting accessibility and reproducibility in LLM reasoning analysis.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.03979v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>FlexInfer: Breaking Memory Constraint via Flexible and Efficient   Offloading for On-Device LLM Inference</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) face challenges for on-device inference due to high memory demands. Traditional methods to reduce memory usage often compromise performance and lack adaptability. We propose FlexInfer, an optimized offloading framework for on-device inference, addressing these issues with techniques like asynchronous prefetching, balanced memory locking, and flexible tensor preservation. These strategies enhance memory efficiency and mitigate I/O bottlenecks, ensuring high performance within user-specified resource constraints. Experiments demonstrate that FlexInfer significantly improves throughput under limited resources, achieving up to 12.5 times better performance than existing methods and facilitating the deployment of large models on resource-constrained devices.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.03777v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Shifting Long-Context LLMs Research from Input to Output</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight a critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04723v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference   Alignment</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is hampered by data scarcity. To address this, we propose a novel approach that $\textit{captures}$ learned preferences from well-aligned English models by implicit rewards and $\textit{transfers}$ them to other languages through iterative training. Specifically, we derive an implicit reward model from the logits of an English DPO-aligned model and its corresponding reference model. This reward model is then leveraged to annotate preference relations in cross-lingual instruction-following pairs, using English instructions to evaluate multilingual responses. The annotated data is subsequently used for multilingual DPO fine-tuning, facilitating preference knowledge transfer from English to other languages. Fine-tuning Llama3 for two iterations resulted in a 12.72% average improvement in Win Rate and a 5.97% increase in Length Control Win Rate across all training languages on the X-AlpacaEval leaderboard. Our findings demonstrate that leveraging existing English-aligned models can enable efficient and effective multilingual preference alignment, significantly reducing the need for extensive multilingual preference data. The code is available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04647v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) excel in reasoning but remain constrained by their Chain-of-Thought (CoT) approach, which struggles with complex tasks requiring more nuanced topological reasoning. We introduce SOLAR, Scalable Optimization of Large-scale Architecture for Reasoning, a framework that dynamically optimizes various reasoning topologies to enhance accuracy and efficiency.   Our Topological Annotation Generation (TAG) system automates topological dataset creation and segmentation, improving post-training and evaluation. Additionally, we propose Topological-Scaling, a reward-driven framework that aligns training and inference scaling, equipping LLMs with adaptive, task-aware reasoning.   SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with Topological Tuning, +9% with Topological Reward, and +10.02% with Hybrid Scaling. It also reduces response length by over 5% for complex problems, lowering inference latency.   To foster the reward system, we train a multi-task Topological Reward Model (M-TRM), which autonomously selects the best reasoning topology and answer in a single pass, eliminating the need for training and inference on multiple single-task TRMs (S-TRMs), thus reducing both training cost and inference latency. In addition, in terms of performance, M-TRM surpasses all S-TRMs, improving accuracy by +10% and rank correlation by +9%.   To the best of our knowledge, SOLAR sets a new benchmark for scalable, high-precision LLM reasoning while introducing an automated annotation process and a dynamic reasoning topology competition mechanism.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04530v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>ToolFuzz -- Automated Agent Tool Testing</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large Language Model (LLM) Agents leverage the advanced reasoning capabilities of LLMs in real-world applications. To interface with an environment, these agents often rely on tools, such as web search or database APIs. As the agent provides the LLM with tool documentation along the user query, the completeness and correctness of this documentation is critical. However, tool documentation is often over-, under-, or ill-specified, impeding the agent's accuracy. Standard software testing approaches struggle to identify these errors as they are expressed in natural language. Thus, despite its importance, there currently exists no automated method to test the tool documentation for agents. To address this issue, we present ToolFuzz, the first method for automated testing of tool documentations. ToolFuzz is designed to discover two types of errors: (1) user queries leading to tool runtime errors and (2) user queries that lead to incorrect agent responses. ToolFuzz can generate a large and diverse set of natural inputs, effectively finding tool description errors at a low false positive rate. Further, we present two straightforward prompt-engineering approaches. We evaluate all three tool testing approaches on 32 common LangChain tools and 35 newly created custom tools and 2 novel benchmarks to further strengthen the assessment. We find that many publicly available tools suffer from underspecification. Specifically, we show that ToolFuzz identifies 20x more erroneous inputs compared to the prompt-engineering approaches, making it a key component for building reliable AI agents.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04479v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Towards Autonomous Reinforcement Learning for Real-World Robotic   Manipulation with Large Language Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Recent advancements in Large Language Models (LLMs) and Visual Language Models (VLMs) have significantly impacted robotics, enabling high-level semantic motion planning applications. Reinforcement Learning (RL), a complementary paradigm, enables agents to autonomously optimize complex behaviors through interaction and reward signals. However, designing effective reward functions for RL remains challenging, especially in real-world tasks where sparse rewards are insufficient and dense rewards require elaborate design. In this work, we propose Autonomous Reinforcement learning for Complex HumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4, a pre-trained LLM, to generate reward functions directly from natural language task descriptions. The rewards are used to train RL agents in simulated environments, where we formalize the reward generation process to enhance feasibility. Additionally, GPT-4 automates the coding of task success criteria, creating a fully automated, one-shot procedure for translating human-readable text into deployable robot skills. Our approach is validated through extensive simulated experiments on single-arm and bi-manual manipulation tasks using an ABB YuMi collaborative robot, highlighting its practicality and effectiveness. Tasks are demonstrated on the real robot setup.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04280v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Knowledge Retention for Continual Model-Based Reinforcement Learning</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">We propose DRAGO, a novel approach for continual model-based reinforcement learning aimed at improving the incremental development of world models across a sequence of tasks that differ in their reward functions but not the state space or dynamics. DRAGO comprises two key components: Synthetic Experience Rehearsal, which leverages generative models to create synthetic experiences from past tasks, allowing the agent to reinforce previously learned dynamics without storing data, and Regaining Memories Through Exploration, which introduces an intrinsic reward mechanism to guide the agent toward revisiting relevant states from prior tasks. Together, these components enable the agent to maintain a comprehensive and continually developing world model, facilitating more effective learning and adaptation across diverse environments. Empirical evaluations demonstrate that DRAGO is able to preserve knowledge across tasks, achieving superior performance in various continual learning scenarios.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04256v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Large-Scale AI in Telecom: Charting the Roadmap for Innovation,   Scalability, and Enhanced Digital Experiences</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">This white paper discusses the role of large-scale AI in the telecommunications industry, with a specific focus on the potential of generative AI to revolutionize network functions and user experiences, especially in the context of 6G systems. It highlights the development and deployment of Large Telecom Models (LTMs), which are tailored AI models designed to address the complex challenges faced by modern telecom networks. The paper covers a wide range of topics, from the architecture and deployment strategies of LTMs to their applications in network management, resource allocation, and optimization. It also explores the regulatory, ethical, and standardization considerations for LTMs, offering insights into their future integration into telecom infrastructure. The goal is to provide a comprehensive roadmap for the adoption of LTMs to enhance scalability, performance, and user-centric innovation in telecom networks.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04184v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Semantic Retrieval Augmented Contrastive Learning for Sequential   Recommendation</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Sequential recommendation aims to model user preferences based on historical behavior sequences, which is crucial for various online platforms. Data sparsity remains a significant challenge in this area as most users have limited interactions and many items receive little attention. To mitigate this issue, contrastive learning has been widely adopted. By constructing positive sample pairs from the data itself and maximizing their agreement in the embedding space,it can leverage available data more effectively. Constructing reasonable positive sample pairs is crucial for the success of contrastive learning. However, current approaches struggle to generate reliable positive pairs as they either rely on representations learned from inherently sparse collaborative signals or use random perturbations which introduce significant uncertainty. To address these limitations, we propose a novel approach named Semantic Retrieval Augmented Contrastive Learning (SRA-CL), which leverages semantic information to improve the reliability of contrastive samples. SRA-CL comprises two main components: (1) Cross-Sequence Contrastive Learning via User Semantic Retrieval, which utilizes large language models (LLMs) to understand diverse user preferences and retrieve semantically similar users to form reliable positive samples through a learnable sample synthesis method; and (2) Intra-Sequence Contrastive Learning via Item Semantic Retrieval, which employs LLMs to comprehend items and retrieve similar items to perform semantic-based item substitution, thereby creating semantically consistent augmented views for contrastive learning. SRA-CL is plug-and-play and can be integrated into standard sequential recommendation models. Extensive experiments on four public datasets demonstrate the effectiveness and generalizability of the proposed approach.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.04162v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>WIP: Assessing the Effectiveness of ChatGPT in Preparatory Testing   Activities</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">This innovative practice WIP paper describes a research study that explores the integration of ChatGPT into the software testing curriculum and evaluates its effectiveness compared to human-generated testing artifacts. In a Capstone Project course, students were tasked with generating preparatory testing artifacts using ChatGPT prompts, which they had previously created manually. Their understanding and the effectiveness of the Artificial Intelligence generated artifacts were assessed through targeted questions. The results, drawn from this in-class assignment at a North American community college indicate that while ChatGPT can automate many testing preparation tasks, it cannot fully replace human expertise. However, students, already familiar with Information Technology at the postgraduate level, found the integration of ChatGPT into their workflow to be straightforward. The study suggests that AI can be gradually introduced into software testing education to keep pace with technological advancements.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.03951v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Learning to Negotiate via Voluntary Commitment</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">The partial alignment and conflict of autonomous agents lead to mixed-motive scenarios in many real-world applications. However, agents may fail to cooperate in practice even when cooperation yields a better outcome. One well known reason for this failure comes from non-credible commitments. To facilitate commitments among agents for better cooperation, we define Markov Commitment Games (MCGs), a variant of commitment games, where agents can voluntarily commit to their proposed future plans. Based on MCGs, we propose a learnable commitment protocol via policy gradients. We further propose incentive-compatible learning to accelerate convergence to equilibria with better social welfare. Experimental results in challenging mixed-motive tasks demonstrate faster empirical convergence and higher returns for our method compared with its counterparts. Our code is available at https://github.com/shuhui-zhu/DCL.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.03866v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream   Impact of Language Model Design Decisions</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Improvements in language model capabilities are often attributed to increasing model size or training data, but in some cases smaller models trained on curated data or with different architectural decisions can outperform larger ones trained on more tokens. What accounts for this? To quantify the impact of these design choices, we meta-analyze 92 open-source pretrained models across a wide array of scales, including state-of-the-art open-weights models as well as less performant models and those with less conventional design decisions. We find that by incorporating features besides model size and number of training tokens, we can achieve a relative 3-28% increase in ability to predict downstream performance compared with using scale alone. Analysis of model design decisions reveal insights into data composition, such as the trade-off between language and code tasks at 15-25\% code, as well as the better performance of some architectural decisions such as choosing rotary over learned embeddings. Broadly, our framework lays a foundation for more systematic investigation of how model development choices shape final capabilities.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.03862v1" target="_blank">View on arXiv</a>
    </div>
</div>

    </div>
</div>

    </body>
    </html>
    