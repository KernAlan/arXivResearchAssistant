
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>ArXiv Research Digest</title>
        <style>
            
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.5;
    color: #2d3748;
    margin: 0;
    padding: 20px;
    background: #f7fafc;
}

.digest {
    max-width: 800px;
    margin: 0 auto;
    background: white;
    padding: 2rem;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.header {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.header h2 {
    margin: 0;
    color: #2d3748;
}

.header p {
    margin: 0.5rem 0 0;
    color: #718096;
}

.highlights {
    background: #f8fafc;
    padding: 1.5rem;
    border-radius: 8px;
    margin-bottom: 3rem;
    border: 1px solid #e2e8f0;
}

.highlights h3 {
    color: #2d3748;
    margin: 0 0 1rem 0;
    font-size: 1.25rem;
}

.summary {
    white-space: pre-line;
    line-height: 1.6;
}

.summary ol {
    padding-left: 1.5rem;
    margin: 1rem 0;
}

.summary li {
    margin-bottom: 0.75rem;
    padding-left: 0.5rem;
}

.papers {
    margin-top: 3rem;
}

.papers h3 {
    color: #2d3748;
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.paper {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid #e2e8f0;
}

.paper:last-child {
    border-bottom: none;
}

.paper h3 {
    margin: 0 0 0.5rem;
    color: #2d3748;
}

.paper .scores {
    display: flex;
    gap: 1rem;
    margin-bottom: 0.5rem;
}

.paper .score {
    padding: 0.25rem 0.5rem;
    border-radius: 4px;
    font-size: 0.875rem;
}

.paper .relevance {
    background: #e6fffa;
    color: #047481;
}

.paper .importance {
    background: #ebf4ff;
    color: #1a56db;
}

.paper .abstract {
    color: #4a5568;
    margin: 0.5rem 0;
}

.paper .meta {
    font-size: 0.875rem;
    color: #718096;
}

.paper .meta a {
    color: #4299e1;
    text-decoration: none;
}

.paper .meta a:hover {
    text-decoration: underline;
}

        </style>
    </head>
    <body>
        
<div class="digest">
    <div class="header">
        <h2>ArXiv Research Digest</h2>
        <p>Found 20 relevant papers out of 200 new submissions</p>
    </div>
    
    <div class="highlights">
        <h3>üî• Key Highlights</h3>
        Hey boss, found 5 cool papers today that I think you'll love:

<ol>
<li>Some researchers tackled the issue of catastrophic forgetting in large language models (LLMs). They‚Äôve developed a full fine-tuning method using adaptive singular value decomposition (SVD) that keeps the model sharp across tasks without needing extra parameters. This could really help in continual learning scenarios where we want to adapt to new info without losing what we already know!</li>

<li>A team came up with a game-changing zero-shot learning approach for monitoring road pavement conditions. By leveraging LLMs and clever prompt engineering, they're able to assess road conditions effectively without needing huge labeled datasets. This could save time and resources in transportation safety and maintenance!</li>

<li>There‚Äôs a new framework called SCARF for assessing Retrieval Augmented Generation (RAG) systems. It provides a comprehensive, black-box evaluation for deployed RAG applications, making it easier for researchers and industry folks to benchmark their systems. This could streamline our evaluations and improve the accuracy of our models in real-world situations.</li>

<li>Another interesting one focuses on enhancing LLMs with neuro-symbolic integration. By combining symbolic reasoning with machine learning, they‚Äôre reducing inaccuracies and logical inconsistencies in model outputs. This is super relevant if we want more reliable responses in critical applications!</li>

<li>Lastly, there's a paper on boosting LLMs for automated reinforcement learning reward design. Their new heuristic framework helps evolve the reward observation space effectively, which could lead to more successful RL applications. Plus, they‚Äôve got demos available to check out!</li>
</ol>

Let me know if you want to dive deeper into any of these! ‚òïÔ∏è
    </div>
    
    <div class="papers">
        <h3>üìÑ Detailed Papers</h3>
        
<div class="paper">
    <h3>Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual   Learning</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 10/10</span>
    </div>
    <div class="abstract">Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07097v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Zero-Shot Image-Based Large Language Model Approach to Road Pavement   Monitoring</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 10/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Effective and rapid evaluation of pavement surface condition is critical for prioritizing maintenance, ensuring transportation safety, and minimizing vehicle wear and tear. While conventional manual inspections suffer from subjectivity, existing machine learning-based methods are constrained by their reliance on large and high-quality labeled datasets, which require significant resources and limit adaptability across varied road conditions. The revolutionary advancements in Large Language Models (LLMs) present significant potential for overcoming these challenges. In this study, we propose an innovative automated zero-shot learning approach that leverages the image recognition and natural language understanding capabilities of LLMs to assess road conditions effectively. Multiple LLM-based assessment models were developed, employing prompt engineering strategies aligned with the Pavement Surface Condition Index (PSCI) standards. These models' accuracy and reliability were evaluated against official PSCI results, with an optimized model ultimately selected. Extensive tests benchmarked the optimized model against evaluations from various levels experts using Google Street View road images. The results reveal that the LLM-based approach can effectively assess road conditions, with the optimized model -employing comprehensive and structured prompt engineering strategies -outperforming simpler configurations by achieving high accuracy and consistency, even surpassing expert evaluations. Moreover, successfully applying the optimized model to Google Street View images demonstrates its potential for future city-scale deployments. These findings highlight the transformative potential of LLMs in automating road damage evaluations and underscore the pivotal role of detailed prompt engineering in achieving reliable assessments.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.06785v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>A System for Comprehensive Assessment of RAG Frameworks</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for enhancing the factual accuracy and contextual relevance of Large Language Models (LLMs) by integrating retrieval mechanisms. However, existing evaluation frameworks fail to provide a holistic black-box approach to assessing RAG systems, especially in real-world deployment scenarios. To address this gap, we introduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a modular and flexible evaluation framework designed to benchmark deployed RAG applications systematically. SCARF provides an end-to-end, black-box evaluation methodology, enabling a limited-effort comparison across diverse RAG frameworks. Our framework supports multiple deployment configurations and facilitates automated testing across vector databases and LLM serving strategies, producing a detailed performance report. Moreover, SCARF integrates practical considerations such as response coherence, providing a scalable and adaptable solution for researchers and industry professionals evaluating RAG applications. Using the REST APIs interface, we demonstrate how SCARF can be applied to real-world scenarios, showcasing its flexibility in assessing different RAG frameworks and configurations. SCARF is available at GitHub repository.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07803v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Enhancing Large Language Models through Neuro-Symbolic Integration and   Ontological Reasoning</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) demonstrate impressive capabilities in natural language processing but suffer from inaccuracies and logical inconsistencies known as hallucinations. This compromises their reliability, especially in domains requiring factual accuracy. We propose a neuro-symbolic approach integrating symbolic ontological reasoning and machine learning methods to enhance the consistency and reliability of LLM outputs. Our workflow utilizes OWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking, and a lightweight machine learning model (logistic regression) for mapping natural language statements into logical forms compatible with the ontology. When inconsistencies between LLM outputs and the ontology are detected, the system generates explanatory feedback to guide the LLM towards a corrected, logically coherent response in an iterative refinement loop. We present a working Python prototype demonstrating this pipeline. Experimental results in a defined domain suggest significant improvements in semantic coherence and factual accuracy of LLM outputs, showcasing the potential of combining LLM fluency with the rigor of formal semantics.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07640v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Boosting Universal LLM Reward Design through the Heuristic Reward   Observation Space Evolution</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting relevant environment states and defining their internal operations. However, existing frameworks have not effectively leveraged historical exploration data or manual task descriptions to iteratively evolve this space. In this paper, we propose a novel heuristic framework that enhances LLM-driven reward design by evolving the ROS through a table-based exploration caching mechanism and a text-code reconciliation strategy. Our framework introduces a state execution table, which tracks the historical usage and success rates of environment states, overcoming the Markovian constraint typically found in LLM dialogues and facilitating more effective exploration. Furthermore, we reconcile user-provided task descriptions with expert-defined success criteria using structured prompts, ensuring alignment in reward design objectives. Comprehensive evaluations on benchmark RL tasks demonstrate the effectiveness and stability of the proposed framework. Code and video demos are available at jingjjjjjie.github.io/LLM2Reward.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07596v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>GPT Carry-On: Training Foundation Model for Customization Could Be   Simple, Scalable and Affordable</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Modern large language foundation models (LLM) have now entered the daily lives of millions of users. We ask a natural question whether it is possible to customize LLM for every user or every task. From system and industrial economy consideration, general continue-training or fine-tuning still require substantial computation and memory of training GPU nodes, whereas most inference nodes under deployment, possibly with lower-end GPUs, are configured to make forward pass fastest possible. We propose a framework to take full advantages of existing LLMs and systems of online service. We train an additional branch of transformer blocks on the final-layer embedding of pretrained LLMs, which is the base, then a carry-on module merge the base models to compose a customized LLM. We can mix multiple layers, or multiple LLMs specialized in different domains such as chat, coding, math, to form a new mixture of LLM that best fit a new task. As the base model don't need to update parameters, we are able to outsource most computation of the training job on inference nodes, and only train a lightweight carry-on on training nodes, where we consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM. We tested Qwen and DeepSeek opensourced models for continue-pretraining and got faster loss convergence. We use it to improve solving math questions with extremely small computation and model size, with 1000 data samples of chain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on, and the results are promising.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07513v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large   Language Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07385v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Modeling Response Consistency in Multi-Agent LLM Systems: A Comparative   Analysis of Shared and Separate Context Approaches</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) are increasingly utilized in multi-agent systems (MAS) to enhance collaborative problem-solving and interactive reasoning. Recent advancements have enabled LLMs to function as autonomous agents capable of understanding complex interactions across multiple topics. However, deploying LLMs in MAS introduces challenges related to context management, response consistency, and scalability, especially when agents must operate under memory limitations and handle noisy inputs. While prior research has explored optimizing context sharing and response latency in LLM-driven MAS, these efforts often focus on either fully centralized or decentralized configurations, each with distinct trade-offs.   In this paper, we develop a probabilistic framework to analyze the impact of shared versus separate context configurations on response consistency and response times in LLM-based MAS. We introduce the Response Consistency Index (RCI) as a metric to evaluate the effects of context limitations, noise, and inter-agent dependencies on system performance. Our approach differs from existing research by focusing on the interplay between memory constraints and noise management, providing insights into optimizing scalability and response times in environments with interdependent topics. Through this analysis, we offer a comprehensive understanding of how different configurations impact the efficiency of LLM-driven multi-agent systems, thereby guiding the design of more robust architectures.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07303v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Better Decisions through the Right Causal World Model</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Reinforcement learning (RL) agents have shown remarkable performances in various environments, where they can discover effective policies directly from sensory inputs. However, these agents often exploit spurious correlations in the training data, resulting in brittle behaviours that fail to generalize to new or slightly modified environments. To address this, we introduce the Causal Object-centric Model Extraction Tool (COMET), a novel algorithm designed to learn the exact interpretable causal world models (CWMs). COMET first extracts object-centric state descriptions from observations and identifies the environment's internal states related to the depicted objects' properties. Using symbolic regression, it models object-centric transitions and derives causal relationships governing object dynamics. COMET further incorporates large language models (LLMs) for semantic inference, annotating causal variables to enhance interpretability.   By leveraging these capabilities, COMET constructs CWMs that align with the true causal structure of the environment, enabling agents to focus on task-relevant features. The extracted CWMs mitigate the danger of shortcuts, permitting the development of RL systems capable of better planning and decision-making across dynamic scenarios. Our results, validated in Atari environments such as Pong and Freeway, demonstrate the accuracy and robustness of COMET, highlighting its potential to bridge the gap between object-centric reasoning and causal inference in reinforcement learning.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07257v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on   Textualized Knowledge Graphs</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Knowledge graphs have emerged as a popular method for injecting up-to-date, factual knowledge into large language models (LLMs). This is typically achieved by converting the knowledge graph into text that the LLM can process in context. While multiple methods of encoding knowledge graphs have been proposed, the impact of this textualization process on LLM performance remains under-explored. We introduce KG-LLM-Bench, a comprehensive and extensible benchmark spanning five knowledge graph understanding tasks, and evaluate how different encoding strategies affect performance across various base models. Our extensive experiments with seven language models and five textualization strategies provide insights for optimizing LLM performance on KG reasoning tasks.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07087v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations,   Architectural Components, and Cognitive Integration</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks. Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. While agents are capable of perceiving their environments, forming inferences, planning, and executing actions towards goals, they often face issues such as hallucinations and lack of contextual memory across interactions. This paper explores how Case-Based Reasoning (CBR), a strategy that solves new problems by referencing past experiences, can be integrated into LLM agent frameworks. This integration allows LLMs to leverage explicit knowledge, enhancing their effectiveness. We systematically review the theoretical foundations of these enhanced agents, identify critical framework components, and formulate a mathematical model for the CBR processes of case retrieval, adaptation, and learning. We also evaluate CBR-enhanced agents against other methods like Chain-of-Thought reasoning and standard Retrieval-Augmented Generation, analyzing their relative strengths. Moreover, we explore how leveraging CBR's cognitive dimensions (including self-reflection, introspection, and curiosity) via goal-driven autonomy mechanisms can further enhance the LLM agent capabilities. Contributing to the ongoing research on neuro-symbolic hybrid systems, this work posits CBR as a viable technique for enhancing the reasoning skills and cognitive aspects of autonomous LLM agents.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.06943v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Bridging the Gap Between Preference Alignment and Machine Unlearning</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Despite advances in Preference Alignment (PA) for Large Language Models (LLMs), mainstream methods like Reinforcement Learning with Human Feedback (RLHF) face notable challenges. These approaches require high-quality datasets of positive preference examples, which are costly to obtain and computationally intensive due to training instability, limiting their use in low-resource scenarios. LLM unlearning technique presents a promising alternative, by directly removing the influence of negative examples. However, current research has primarily focused on empirical validation, lacking systematic quantitative analysis. To bridge this gap, we propose a framework to explore the relationship between PA and LLM unlearning. Specifically, we introduce a bi-level optimization-based method to quantify the impact of unlearning specific negative examples on PA performance. Our analysis reveals that not all negative examples contribute equally to alignment improvement when unlearned, and the effect varies significantly across examples. Building on this insight, we pose a crucial question: how can we optimally select and weight negative examples for unlearning to maximize PA performance? To answer this, we propose a framework called Unlearning to Align (U2A), which leverages bi-level optimization to efficiently select and unlearn examples for optimal PA performance. We validate the proposed method through extensive experiments, with results confirming its effectiveness.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.06659v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>ARLO: A Tailorable Approach for Transforming Natural Language Software   Requirements into Architecture using LLMs</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Software requirements expressed in natural language (NL) frequently suffer from verbosity, ambiguity, and inconsistency. This creates a range of challenges, including selecting an appropriate architecture for a system and assessing different architectural alternatives. Relying on human expertise to accomplish the task of mapping NL requirements to architecture is time-consuming and error-prone. This paper proposes ARLO, an approach that automates this task by leveraging (1) a set of NL requirements for a system, (2) an existing standard that specifies architecturally relevant software quality attributes, and (3) a readily available Large Language Model (LLM). Specifically, ARLO determines the subset of NL requirements for a given system that is architecturally relevant and maps that subset to a tailorable matrix of architectural choices. ARLO applies integer linear programming on the architectural-choice matrix to determine the optimal architecture for the current requirements. We demonstrate ARLO's efficacy using a set of real-world examples. We highlight ARLO's ability (1) to trace the selected architectural choices to the requirements and (2) to isolate NL requirements that exert a particular influence on a system's architecture. This allows the identification, comparative assessment, and exploration of alternative architectural choices based on the requirements and constraints expressed therein.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.06143v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Fast Adaptation with Behavioral Foundation Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful paradigm for pretraining behavioral foundation models (BFMs), enabling agents to solve a wide range of downstream tasks specified via reward functions in a zero-shot fashion, i.e., without additional test-time learning or planning. This is achieved by learning self-supervised task embeddings alongside corresponding near-optimal behaviors and incorporating an inference procedure to directly retrieve the latent task embedding and associated policy for any given reward function. Despite promising results, zero-shot policies are often suboptimal due to errors induced by the unsupervised training process, the embedding, and the inference procedure. In this paper, we focus on devising fast adaptation strategies to improve the zero-shot performance of BFMs in a few steps of online interaction with the environment while avoiding any performance drop during the adaptation process. Notably, we demonstrate that existing BFMs learn a set of skills containing more performant policies than those identified by their inference procedure, making them well-suited for fast adaptation. Motivated by this observation, we propose both actor-critic and actor-only fast adaptation strategies that search in the low-dimensional task-embedding space of the pre-trained BFM to rapidly improve the performance of its zero-shot policies on any downstream task. Notably, our approach mitigates the initial "unlearning" phase commonly observed when fine-tuning pre-trained RL models. We evaluate our fast adaptation strategies on top of four state-of-the-art zero-shot RL methods in multiple navigation and locomotion domains. Our results show that they achieve 10-40% improvement over their zero-shot performance in a few tens of episodes, outperforming existing baselines.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07896v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Dual Engines of Thoughts: A Depth-Breadth Integration Framework for   Open-Ended Analysis</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">We propose the Dual Engines of Thoughts (DEoT), an analytical framework for comprehensive open-ended reasoning. While traditional reasoning frameworks primarily focus on finding "the best answer" or "the correct answer" for single-answer problems, DEoT is specifically designed for "open-ended questions," enabling both broader and deeper analytical exploration. The framework centers on three key components: a Base Prompter for refining user queries, a Solver Agent that orchestrates task decomposition, execution, and validation, and a Dual-Engine System consisting of a Breadth Engine (to explore diverse impact factors) and a Depth Engine (to perform deep investigations). This integrated design allows DEoT to balance wide-ranging coverage with in-depth analysis, and it is highly customizable, enabling users to adjust analytical parameters and tool configurations based on specific requirements. Experimental results show that DEoT excels in addressing complex, multi-faceted questions, achieving a total win rate of 77-86% compared to existing reasoning models, thus highlighting its effectiveness in real-world applications.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07872v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend   NPUs</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07866v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference   Optimization</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Aligning large language models with human preferences is crucial for their safe deployment. While Direct Preference Optimization (DPO) offers an efficient alternative to reinforcement learning from human feedback, traditional DPO methods are limited by their reliance on single preference pairs. Recent work like Curriculum-DPO integrates multiple pairs using a one-dimensional difficulty curriculum based on pairwise distinguishability (PD), but overlooks the complexity of the input prompt itself. To address this, we propose 2D-Curri-DPO, a novel framework employing a two-dimensional curriculum that jointly models Prompt Complexity (PC) and Pairwise Distinguishability. This framework introduces dual difficulty metrics to quantify prompt semantic complexity and response preference clarity, defines a curriculum strategy space encompassing multiple selectable strategies for task adaptation, and incorporates a KL-divergence-based adaptive mechanism for dynamic reference model updates to enhance training stability. Comprehensive experiments demonstrate that 2D-Curri-DPO significantly outperforms standard DPO and prior curriculum methods across multiple benchmarks, including MT-Bench, Vicuna Bench, and WizardLM. Our approach achieves state-of-the-art performance on challenging test sets like UltraFeedback. Ablation studies confirm the benefits of the 2D structure and adaptive mechanisms, while analysis provides guidance for strategy selection. These findings demonstrate that effective alignment requires modeling both prompt complexity and pairwise distinguishability, establishing adaptive, multi-dimensional curriculum learning as a powerful and interpretable new paradigm for preference-based language model optimization.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07856v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Understanding Learner-LLM Chatbot Interactions and the Impact of   Prompting Guidelines</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) have transformed human-computer interaction by enabling natural language-based communication with AI-powered chatbots. These models are designed to be intuitive and user-friendly, allowing users to articulate requests with minimal effort. However, despite their accessibility, studies reveal that users often struggle with effective prompting, resulting in inefficient responses. Existing research has highlighted both the limitations of LLMs in interpreting vague or poorly structured prompts and the difficulties users face in crafting precise queries. This study investigates learner-AI interactions through an educational experiment in which participants receive structured guidance on effective prompting. We introduce and compare three types of prompting guidelines: a task-specific framework developed through a structured methodology and two baseline approaches. To assess user behavior and prompting efficacy, we analyze a dataset of 642 interactions from 107 users. Using Von NeuMidas, an extended pragmatic annotation schema for LLM interaction analysis, we categorize common prompting errors and identify recurring behavioral patterns. We then evaluate the impact of different guidelines by examining changes in user behavior, adherence to prompting strategies, and the overall quality of AI-generated responses. Our findings provide a deeper understanding of how users engage with LLMs and the role of structured prompting guidance in enhancing AI-assisted communication. By comparing different instructional frameworks, we offer insights into more effective approaches for improving user competency in AI interactions, with implications for AI literacy, chatbot usability, and the design of more responsive AI systems.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07840v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented   Generation in Large Language Models via Bilevel Optimization</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database, (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07717v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Synthesizing High-Quality Programming Tasks with LLM-based Expert and   Student Agents</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Generative AI is transforming computing education by enabling the automatic generation of personalized content and feedback. We investigate its capabilities in providing high-quality programming tasks to students. Despite promising advancements in task generation, a quality gap remains between AI-generated and expert-created tasks. The AI-generated tasks may not align with target programming concepts, could be incomprehensible for students to solve, or may contain critical issues such as incorrect tests. Existing works often require interventions from human teachers for validation. We address these challenges by introducing PyTaskSyn, a novel synthesis technique that first generates a programming task and then decides whether it meets certain quality criteria to be given to students. The key idea is to break this process into multiple stages performed by expert and student agents simulated using both strong and weaker generative models. Through extensive evaluation, we show that PyTaskSyn significantly improves task quality compared to baseline techniques and showcases the importance of each specialized agent type in our validation pipeline. Additionally, we conducted user studies using our publicly available web application and show that PyTaskSyn can deliver high-quality programming tasks comparable to expert-designed ones while reducing workload and costs, and being more engaging than programming tasks that are available in online resources.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2504.07655v1" target="_blank">View on arXiv</a>
    </div>
</div>

    </div>
</div>

    </body>
    </html>
    