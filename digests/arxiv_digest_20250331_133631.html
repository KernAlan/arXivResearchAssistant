
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>ArXiv Research Digest</title>
        <style>
            
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.5;
    color: #2d3748;
    margin: 0;
    padding: 20px;
    background: #f7fafc;
}

.digest {
    max-width: 800px;
    margin: 0 auto;
    background: white;
    padding: 2rem;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.header {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.header h2 {
    margin: 0;
    color: #2d3748;
}

.header p {
    margin: 0.5rem 0 0;
    color: #718096;
}

.highlights {
    background: #f8fafc;
    padding: 1.5rem;
    border-radius: 8px;
    margin-bottom: 3rem;
    border: 1px solid #e2e8f0;
}

.highlights h3 {
    color: #2d3748;
    margin: 0 0 1rem 0;
    font-size: 1.25rem;
}

.summary {
    white-space: pre-line;
    line-height: 1.6;
}

.summary ol {
    padding-left: 1.5rem;
    margin: 1rem 0;
}

.summary li {
    margin-bottom: 0.75rem;
    padding-left: 0.5rem;
}

.papers {
    margin-top: 3rem;
}

.papers h3 {
    color: #2d3748;
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 2px solid #e2e8f0;
}

.paper {
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid #e2e8f0;
}

.paper:last-child {
    border-bottom: none;
}

.paper h3 {
    margin: 0 0 0.5rem;
    color: #2d3748;
}

.paper .scores {
    display: flex;
    gap: 1rem;
    margin-bottom: 0.5rem;
}

.paper .score {
    padding: 0.25rem 0.5rem;
    border-radius: 4px;
    font-size: 0.875rem;
}

.paper .relevance {
    background: #e6fffa;
    color: #047481;
}

.paper .importance {
    background: #ebf4ff;
    color: #1a56db;
}

.paper .abstract {
    color: #4a5568;
    margin: 0.5rem 0;
}

.paper .meta {
    font-size: 0.875rem;
    color: #718096;
}

.paper .meta a {
    color: #4299e1;
    text-decoration: none;
}

.paper .meta a:hover {
    text-decoration: underline;
}

        </style>
    </head>
    <body>
        
<div class="digest">
    <div class="header">
        <h2>ArXiv Research Digest</h2>
        <p>Found 20 relevant papers out of 115 new submissions</p>
    </div>
    
    <div class="highlights">
        <h3>ðŸ”¥ Key Highlights</h3>
        Hey boss, I came across 5 really cool papers today that I think you'll love:

<ol>
<li>**Evaluating LLM-based Agents for Multi-Turn Conversations** - This survey from a team of researchers digs into how we should evaluate LLM agents in chats. Theyâ€™ve created a structured framework that covers everything from response quality to context retention. Itâ€™s super important since it sets the standard for measuring conversational agents' performance in a meaningful way, which can help us build better AI interactions.</li>

<li>**RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models** - Researchers introduced a nifty framework that turns general multimodal LLMs into personalized assistants. By remembering user info and augmenting responses based on that, it enhances user engagement and allows real-time updates. This could really transform how users interact with AI in daily tasks!</li>

<li>**QuestBench: Can LLMs Ask the Right Question?** - This paper presents a unique benchmark to test LLMs on their ability to ask the right clarifying questions for reasoning tasks. It highlights that even top models struggle with underspecified queries, showing a clear gap in information acquisition skills. Understanding this could guide future improvements in LLM design.</li>

<li>**Unicorn: Text-Only Data Synthesis for Vision Language Model Training** - This research proposes a method to create high-quality multimodal training data just from text, avoiding the costly need for image-text pairs. Their three-stage framework generates diverse datasets for training vision-language models, making it a game-changer for scalable AI training without breaking the bank.</li>

<li>**A Framework for Cryptographic Verifiability of End-to-End AI Pipelines** - This paper introduces a framework that ensures transparency and trust in AI systems through cryptographic verifiability. With increasing concerns around AI safety, this could be crucial for developing reliable systems that allow for downstream verification of AI-generated outputs.</li>
</ol>

Let me know if you want to dive deeper into any of these!
    </div>
    
    <div class="papers">
        <h3>ðŸ“„ Detailed Papers</h3>
        
<div class="paper">
    <h3>Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 10/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \emph{what to evaluate} and another that explains \emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22458v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>RAP: Retrieval-Augmented Personalization for Multimodal Large Language   Models</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs' personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, e.g., user's name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts' information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models are available at https://hoar012.github.io/RAP-Project/.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2410.13360v3" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>QuestBench: Can LLMs ask the right question to acquire information in   reasoning tasks?</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Recently, a large amount of work has focused on improving large language models' (LLMs') performance on reasoning benchmarks such as math and logic. However, past work has largely assumed that tasks are well-defined. In the real world, queries to LLMs are often underspecified, only solvable through acquiring missing information. We formalize this as a constraint satisfaction problem (CSP) with missing variable assignments. Using a special case of this formalism where only one necessary variable assignment is missing, we can rigorously evaluate an LLM's ability to identify the minimal necessary question to ask and quantify axes of difficulty levels for each problem. We present QuestBench, a set of underspecified reasoning tasks solvable by asking at most one question, which includes: (1) Logic-Q: Logical reasoning tasks with one missing proposition, (2) Planning-Q: PDDL planning problems with initial states that are partially-observed, (3) GSM-Q: Human-annotated grade school math problems with one missing variable assignment, and (4) GSME-Q: a version of GSM-Q where word problems are translated into equations by human annotators. The LLM is tasked with selecting the correct clarification question(s) from a list of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their accuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that the ability to solve well-specified reasoning problems may not be sufficient for success on our benchmark: models have difficulty identifying the right question to ask, even when they can solve the fully specified version of the problem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even when explicitly presented with the option to predict ``not sure.'' This highlights the need for deeper investigation into models' information acquisition capabilities.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22674v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Unicorn: Text-Only Data Synthesis for Vision Language Model Training</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22655v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>A Framework for Cryptographic Verifiability of End-to-End AI Pipelines</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">The increasing integration of Artificial Intelligence across multiple industry sectors necessitates robust mechanisms for ensuring transparency, trust, and auditability of its development and deployment. This topic is particularly important in light of recent calls in various jurisdictions to introduce regulation and legislation on AI safety. In this paper, we propose a framework for complete verifiable AI pipelines, identifying key components and analyzing existing cryptographic approaches that contribute to verifiability across different stages of the AI lifecycle, from data sourcing to training, inference, and unlearning. This framework could be used to combat misinformation by providing cryptographic proofs alongside AI-generated assets to allow downstream verification of their provenance and correctness. Our findings underscore the importance of ongoing research to develop cryptographic tools that are not only efficient for isolated AI processes, but that are efficiently `linkable' across different processes within the AI pipeline, to support the development of end-to-end verifiable AI technologies.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22573v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Niyama : Breaking the Silos of LLM Inference Serving</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">The widespread adoption of Large Language Models (LLMs) has enabled diverse applications with very different latency requirements. Existing LLM serving frameworks rely on siloed infrastructure with coarse-grained workload segregation -- interactive and batch -- leading to inefficient resource utilization and limited support for fine-grained Quality-of-Service (QoS) differentiation. This results in operational inefficiencies, over-provisioning and poor load management during traffic surges.   We present Niyama, a novel QoS-driven inference serving system that enables efficient co-scheduling of diverse workloads on shared infrastructure. Niyama introduces fine-grained QoS classification allowing applications to specify precise latency requirements, and dynamically adapts scheduling decisions based on real-time system state. Leveraging the predictable execution characteristics of LLM inference, Niyama implements a dynamic chunking mechanism to improve overall throughput while maintaining strict QoS guarantees. Additionally, Niyama employs a hybrid prioritization policy that balances fairness and efficiency, and employs selective request relegation that enables graceful service degradation during overload conditions. Our evaluation demonstrates that Niyama increases serving capacity by 32% compared to current siloed deployments, while maintaining QoS guarantees. Notably, under extreme load, our system reduces SLO violations by an order of magnitude compared to current strategies.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22562v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative   Abilities</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">In this work, we undertake the challenge of augmenting the existing generative capabilities of pre-trained text-only large language models (LLMs) with multi-modal generation capability while satisfying two core constraints: C1 preserving the preservation of original language generative capabilities with negligible performance degradation, and C2 adhering to a small parameter budget to learn the new modality, ensuring scalability and efficiency. In contrast to current approaches that add dedicated modules, thereby significantly increasing the parameter count, we propose a method that leverages the underutilized capacity inherent in deep models. Specifically, we exploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source of additional capacity for learning a new modality, enabling better parameter efficiency (C1). Moreover, we preserve the original language generation capabilities by applying low-rank adaptation exclusively to the tokens of the new modality (C2). Furthermore, we introduce a novel parameter initialization scheme based on the Gromov-Wasserstein distance to improve convergence and training stability. Through an extensive analysis of the routing mechanism, we uncover the emergence of modality-specific pathways and decreased redundancy within the experts that can efficiently unlock multi-modal generative capabilities. Overall, our method can be seamlessly applied to a wide range of contemporary LLMs, providing a new pathway for transitioning from uni-modal to multi-modal architectures.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22517v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Firm or Fickle? Evaluating Large Language Models Consistency in   Sequential Interactions</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent performance across multiple interaction rounds. This paper introduces a comprehensive framework for evaluating and improving LLM response consistency, making three key contributions. First, we propose a novel Position-Weighted Consistency (PWC) score that captures both the importance of early-stage stability and recovery patterns in multi-turn interactions. Second, we present a carefully curated benchmark dataset spanning diverse domains and difficulty levels, specifically designed to evaluate LLM consistency under various challenging follow-up scenarios. Third, we introduce Confidence-Aware Response Generation (CARG), a framework that significantly improves response stability by incorporating model confidence signals into the generation process. Empirical results demonstrate that CARG significantly improves response stability without sacrificing accuracy, underscoring its potential for reliable LLM deployment in critical applications.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22353v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Personalized multiple clustering aims to generate diverse partitions of a dataset based on different user-specific aspects, rather than a single clustering. It has recently drawn research interest for accommodating varying user preferences. Recent approaches primarily use CLIP embeddings with proxy learning to extract representations biased toward user clustering preferences. However, CLIP primarily focuses on coarse image-text alignment, lacking a deep contextual understanding of user interests. To overcome these limitations, we propose an agent-centric personalized clustering framework that leverages multi-modal large language models (MLLMs) as agents to comprehensively traverse a relational graph to search for clusters based on user interests. Due to the advanced reasoning mechanism of MLLMs, the obtained clusters align more closely with user-defined criteria than those obtained from CLIP-based representations. To reduce computational overhead, we shorten the agents' traversal path by constructing a relational graph using user-interest-biased embeddings extracted by MLLMs. A large number of weakly connected edges can be filtered out based on embedding similarity, facilitating an efficient traversal search for agents. Experimental results show that the proposed method achieves NMI scores of 0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively, largely improving the SOTA model by over 140%.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22241v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Sell It Before You Make It: Revolutionizing E-Commerce with Personalized   AI-Generated Items</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">E-commerce has revolutionized retail, yet its traditional workflows remain inefficient, with significant time and resource costs tied to product design and manufacturing inventory. This paper introduces a novel system deployed at Alibaba that leverages AI-generated items (AIGI) to address these challenges with personalized text-to-image generation for e-commercial product design. AIGI enables an innovative business mode called "sell it before you make it", where merchants can design fashion items and generate photorealistic images with digital models based on textual descriptions. Only when the items have received a certain number of orders, do the merchants start to produce them, which largely reduces reliance on physical prototypes and thus accelerates time to market. For such a promising application, we identify the underlying key scientific challenge, i.e., capturing the users' group-level personalized preferences towards multiple generated candidate images. To this end, we propose a Personalized Group-Level Preference Alignment Framework for Diffusion Models (i.e., PerFusion). We first design PerFusion Reward Model for user preference estimation with a feature-crossing-based personalized plug-in. Then we develop PerFusion with a personalized adaptive network to model diverse preferences across users, and meanwhile derive the group-level preference optimization objective to capture the comparative behaviors among multiple candidates. Both offline and online experiments demonstrate the effectiveness of our proposed algorithm. The AI-generated items have achieved over 13% relative improvements for both click-through rate and conversion rate compared to their human-designed counterparts, validating the revolutionary potential of AI-generated items for e-commercial platforms.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22182v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>PharmAgents: Building a Virtual Pharma with Large Language Model Agents</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">The discovery of novel small molecule drugs remains a critical scientific challenge with far-reaching implications for treating diseases and advancing human health. Traditional drug development--especially for small molecule therapeutics--is a highly complex, resource-intensive, and time-consuming process that requires multidisciplinary collaboration. Recent breakthroughs in artificial intelligence (AI), particularly the rise of large language models (LLMs), present a transformative opportunity to streamline and accelerate this process. In this paper, we introduce PharmAgents, a virtual pharmaceutical ecosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates the full drug discovery workflow--from target discovery to preclinical evaluation--by integrating explainable, LLM-driven agents equipped with specialized machine learning models and computational tools. Through structured knowledge exchange and automated optimization, PharmAgents identifies potential therapeutic targets, discovers promising lead compounds, enhances binding affinity and key molecular properties, and performs in silico analyses of toxicity and synthetic feasibility. Additionally, the system supports interpretability, agent interaction, and self-evolvement, enabling it to refine future drug designs based on prior experience. By showcasing the potential of LLM-powered multi-agent systems in drug discovery, this work establishes a new paradigm for autonomous, explainable, and scalable pharmaceutical research, with future extensions toward comprehensive drug lifecycle management.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22164v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks   with Dialogues and Stories</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Evaluating the value alignment of large language models (LLMs) has traditionally relied on single-sentence adversarial prompts, which directly probe models with ethically sensitive or controversial questions. However, with the rapid advancements in AI safety techniques, models have become increasingly adept at circumventing these straightforward tests, limiting their effectiveness in revealing underlying biases and ethical stances. To address this limitation, we propose an upgraded value alignment benchmark that moves beyond single-sentence prompts by incorporating multi-turn dialogues and narrative-based scenarios. This approach enhances the stealth and adversarial nature of the evaluation, making it more robust against superficial safeguards implemented in modern LLMs. We design and implement a dataset that includes conversational traps and ethically ambiguous storytelling, systematically assessing LLMs' responses in more nuanced and context-rich settings. Experimental results demonstrate that this enhanced methodology can effectively expose latent biases that remain undetected in traditional single-shot evaluations. Our findings highlight the necessity of contextual and dynamic testing for value alignment in LLMs, paving the way for more sophisticated and realistic assessments of AI ethics and safety.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22115v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A   General Framework for Domain-Specific Large Language Model Adaptation</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Large language models (LLMs) hold great promise for specialized scientific domains such as materials science, yet adapting them efficiently and accurately to domain-specific knowledge remains challenging due to limited data and high knowledge density. We propose a two-stage framework that combines structured model compression with a scientific fine-tuning regimen to address this challenge. In the compression stage, we decompose the LLM's weight matrices into local low-rank "rank blocks" and arrange these blocks in a Penrose-like non-periodic tiling pattern. Each block is then compacted via spectral transformations (e.g., discrete cosine or Fourier transforms), and a Kullback-Leibler (KL) divergence-based alignment loss preserves the distributional similarity between the compressed model's representations and those of the original full model. In the adaptation stage, the compressed model is further tuned using a human-like scientific reading protocol: it processes technical materials science documents section by section, engaging in a structured question-and-answer routine for each section. This section-wise Q&A fine-tuning strategy extracts explicit reasoning traces and gradually injects domain knowledge, while minimizing catastrophic forgetting of the model's general language capabilities. By balancing efficient compression with targeted adaptation, our two-stage approach enables precise specialization of LLMs to high-value domains under data-scarce conditions. We present this principled yet exploratory pipeline and outline its potential for advancing materials science knowledge integration, laying the groundwork for comprehensive empirical evaluation in future work.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22074v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Overtrained Language Models Are Harder to Fine-Tune</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large language models are pre-trained on ever-growing token budgets under the assumption that better pre-training performance translates to improved downstream models. In this work, we challenge this assumption and show that extended pre-training can make models harder to fine-tune, leading to degraded final performance. We term this phenomenon catastrophic overtraining. For example, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads to over 2% worse performance on multiple standard LLM benchmarks than its 2.3T token counterpart. Through controlled experiments and theoretical analysis, we show that catastrophic overtraining arises from a systematic increase in the broad sensitivity of pre-trained parameters to modifications, including but not limited to fine-tuning. Our findings call for a critical reassessment of pre-training design that considers the downstream adaptability of the model.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.19206v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Can Language Models Follow Multiple Turns of Entangled Instructions?</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Despite significant achievements in improving the instruction-following capabilities of large language models (LLMs), the ability to process multiple potentially entangled or conflicting instructions remains a considerable challenge. Real-world scenarios often require consistency across multiple instructions over time, such as secret privacy, personal preferences, and prioritization, which demand sophisticated abilities to integrate multiple turns and carefully balance competing objectives when instructions intersect or conflict. This work presents a systematic investigation of LLMs' capabilities in handling multiple turns of instructions, covering three levels of difficulty: (1) retrieving information from instructions, (2) tracking and reasoning across turns, and (3) resolving conflicts among instructions. We construct MultiTurnInstruct with around 1.1K high-quality multi-turn conversations through the human-in-the-loop approach and result in nine capability categories, including statics and dynamics, reasoning, and multitasking. Our finding reveals an intriguing trade-off between different capabilities. While GPT models demonstrate superior memorization, they show reduced effectiveness in privacy-protection tasks requiring selective information withholding. Larger models exhibit stronger reasoning capabilities but still struggle with resolving conflicting instructions. Importantly, these performance gaps cannot be attributed solely to information loss, as models demonstrate strong BLEU scores on memorization tasks but their attention mechanisms fail to integrate multiple related instructions effectively. These findings highlight critical areas for improvement in complex real-world tasks involving multi-turn instructions.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.13222v2" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for   Boosting 2-bit Large Language Model Accuracy</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 9/10</span>
    </div>
    <div class="abstract">Low-rank adaptation (LoRA) has become the dominant method for parameter-efficient LLM fine-tuning, with LoRA-based quantization error compensation (LQEC) emerging as a powerful tool for recovering accuracy in compressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, with no prior investigation into understanding this limitation. We propose RILQ (Rank-Insensitive LoRA-based Quantization Error Compensation) to understand fundamental limitation and boost 2-bit LLM accuracy. Based on rank analysis revealing model-wise activation discrepancy loss's rank-insensitive nature, RILQ employs this loss to adjust adapters cooperatively across layers, enabling robust error compensation with low-rank adapters. Evaluations on LLaMA-2 and LLaMA-3 demonstrate RILQ's consistent improvements in 2-bit quantized inference across various state-of-the-art quantizers and enhanced accuracy in task-specific fine-tuning. RILQ maintains computational efficiency comparable to existing LoRA methods, enabling adapter-merged weight-quantized LLM inference with significantly enhanced accuracy, making it a promising approach for boosting 2-bit LLM performance. Our code is available at https://github.com/aiha-lab/RILQ.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2412.01129v3" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Do LLMs estimate uncertainty well in instruction-following?</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. Our study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models. To address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions. Our findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. The insights from our controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2410.14582v4" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and   Adaptation</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 9/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">Large language models (LLMs) are a transformational capability at the frontier of artificial intelligence and machine learning that can support decision-makers in addressing pressing societal challenges such as extreme natural hazard events. As generalized models, LLMs often struggle to provide context-specific information, particularly in areas requiring specialized knowledge. In this work, we propose a Retrieval-Augmented Generation (RAG)-based multi-agent LLM system to support analysis and decision-making in the context of natural hazards and extreme weather events. As a proof of concept, we present WildfireGPT, a specialized system focused on wildfire scenarios. The architecture employs a user-centered, multi-agent design to deliver tailored risk insights across diverse stakeholder groups. By integrating domain-specific projection data, observational datasets, and scientific literature through a RAG framework, the system ensures both accuracy and contextual relevance of the information it provides. Evaluation across ten expert-led case studies demonstrates that WildfireGPT significantly outperforms existing LLM-based solutions for decision support in natural hazard and extreme weather contexts.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2402.07877v3" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Challenges and Paths Towards AI for Software Engineering</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 7/10</span>
        <span class="score importance">Importance: 8/10</span>
    </div>
    <div class="abstract">AI for software engineering has made remarkable progress recently, becoming a notable success within generative AI. Despite this, there are still many challenges that need to be addressed before automated software engineering reaches its full potential. It should be possible to reach high levels of automation where humans can focus on the critical decisions of what to build and how to balance difficult tradeoffs while most routine development effort is automated away. Reaching this level of automation will require substantial research and engineering efforts across academia and industry. In this paper, we aim to discuss progress towards this in a threefold manner. First, we provide a structured taxonomy of concrete tasks in AI for software engineering, emphasizing the many other tasks in software engineering beyond code generation and completion. Second, we outline several key bottlenecks that limit current approaches. Finally, we provide an opinionated list of promising research directions toward making progress on these bottlenecks, hoping to inspire future research in this rapidly maturing field.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22625v1" target="_blank">View on arXiv</a>
    </div>
</div>


<div class="paper">
    <h3>Entropy-guided sequence weighting for efficient exploration in RL-based   LLM fine-tuning</h3>
    <div class="scores">
        <span class="score relevance">Relevance: 8/10</span>
        <span class="score importance">Importance: 7/10</span>
    </div>
    <div class="abstract">We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that enhances the exploration-exploitation tradeoff by dynamically assigning weights to generated outputs based on their advantage and entropy for Reinforcement Learning-based Large Language Model fine-tuning. EGSW integrates entropy regularization with advantage-based weighting to balance policy updates, enabling efficient exploration in high-dimensional state spaces. By employing temperature-scaled softmax weighting over sequences, EGSW prioritizing high-reward, high-uncertainty steps while maintaining training stability. Although originally developed to improve Group Relative Policy Optimization (GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to other reinforcement learning (RL) algorithms and can be implemented in both step-wise and trajectory-wise settings. Empirical evaluations demonstrate that EGSW enhances GRPO reasoning ability, yielding improvements in sample efficiency. Future work will explore the application of EGSW to advanced RL methodologies.</div>
    <div class="meta">
        <a href="http://arxiv.org/abs/2503.22456v1" target="_blank">View on arXiv</a>
    </div>
</div>

    </div>
</div>

    </body>
    </html>
    